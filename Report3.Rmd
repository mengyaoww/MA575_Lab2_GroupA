---
title: "Report3"
subtitle: "Lab2 Group A: Lee, Joshua; Liu, Kaiyi; Pulsone, Nathaniel; Wang, Mengyao; Xu, Zexian; Yang, Xiaojing"
output: pdf_document
subparagraph: yes
header-includes:
  \usepackage{titlesec}
  \titlespacing{\title}{0pt}{\parskip}{-\parskip}
  \usepackage{titling}
  \setlength{\droptitle}{-10em}
---

\vspace{-10truemm}

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,    # Show the R code in the final output.
  fig.pos = "!h"  # Suggests to position figures "here" in the output.
)
```

### Loading the Library and DataSet

```{r message=FALSE}
mylibrary <- c("tidyverse", "cowplot", "GGally", "MASS", "ggplot2", "glmnet", "data.table", "car")
invisible(lapply(mylibrary, library, character.only = TRUE))
### load the data
dat_bmw <- read.csv("BMWpricing_updated.csv")
```

### Data Overview

Sourced from Kaggle, the dataset provides information on roughly 5000 used BMW cars sold in a business-to-business auction. Notable variables included are the price, car model, color, mileage, engine power, and various categorical descriptors.

```{r, echo = F}
par(mfrow=c(2,2))
dat_bmw[,c("price", "mileage", "engine_power")]|>
  pivot_longer(everything(), values_to = "Value", names_to = "Variable") |>
  ggplot() + geom_boxplot(aes(x=Variable, y=Value, color=Variable)) +
  labs(title = "Boxplots for Continuous Variables (Raw)")

dat_bmw|>
  ggplot() + geom_density(aes(x=price)) +
  labs(title = "Density of the Variable 'Price'")

dat_bmw|>
  ggplot(aes(x = as.Date(registration_date, format = "%m/%d/%Y"), y = price)) +
  geom_point() + 
  labs(title = "Price by Vehicle Registration Date") +
  xlab("Registration Date")

dat_bmw|>
  ggplot(aes(x = as.Date(sold_at, format = "%m/%d/%Y"))) +
  geom_bar() + 
  labs(title = "Sales by Date of Auction") +
  xlab("Date Sold") +
  ylab("Number of Cars Auctioned")
  
```

From the graphs above, along with analysis that can be found in the Appendix section, we can see that the cars from the auction were all registered between March 1990 and November 2017, and the auction took place from January to September 2018. The cars spanned 75 different BMW models, 10 different colors, and 4 different fuel types.

The pricing of the cars is most concentrated around 15000, with a median price of 14200. The distribution of price is skewed heavily to the right. Because price is our main response variable, we will see that this causes the residuals of the constructed models to be right-skewed as well. The skewness of the variables can be somewhat remedied with a log-transformation on price.

Although it is hard to identify the feature variables in the dataset, we can see That most of the vehicles posses features 2 and 7, while most do not have features 3, 4 and 6. Each of the features when present tend to increase the price by `$5000` to `$10000`, except for feature 7, which actually decreases the price by about `$185`

Lastly, there are a few unusual values and outliers to consider in the dataset. There are two cars in the dataset that were sold for more than `$100,000` which is unusual enough to provide a high leverage and skew our constructed models. For this reason, we will remove that observation from the dataset. In addition, there is a single car with over 1 million miles on it, which we speculate was an error in data entry, so we will also remove this from the dataset. Finally, there are a number of observations with a negative mileage, or an engine power of zero. These values are also either errors in data entry, or indicative of a special case, such as scrapped or salvaged car. For this reason, we will not remove the observations, but instead set the negative and zero values to NA.

```{r}
dat_bmw_clean <- dat_bmw|>
  filter(mileage < 500000 & mileage > 0)|>
  filter(price < 100000)|>
  filter(engine_power != 0)
  
head(dat_bmw_clean)
```

```{R}
### Create Model series variable
dat_bmw_clean <- dat_bmw_clean %>%
  mutate(model_series = case_when(
    grepl("^1", model_key) ~ "1_Series",
    grepl("^2", model_key) ~ "2_Series",
    grepl("^3", model_key) ~ "3_Series",
    grepl("^4", model_key) ~ "4_Series",
    grepl("^5", model_key) ~ "5_Series",
    grepl("^7", model_key) ~ "7_Series",
    grepl("^M|M$", model_key) ~ "M_Power",
    model_key %in% c("X1") ~ "X1",
    model_key %in% c("X3") ~ "X3",
    model_key %in% c("X5") ~ "X5",
    model_key %in% c("X6") ~ "X6",
    TRUE ~ "Other"
  ))
```

### Model

Q1: Which factor among mileage, engine power, color, model, and other categorical variables impacts the price of the car the most?

We have selected a set of variables: Mileage(numeric), Engine Power(numeric), color(categorical factor), Model(categorical factor), and other categorical factors.

We decided to fit a multiple linear regression. Price = $\beta_0$ + $\beta_1$(Mileage) + $\beta_2$(Engine Power) + ... = e,

The assumptions are: 1) Linearity.The relationship between each numeric predictor and the price is assumed to be linear. 2)Independence of Observations. Each observations should be independent of others. 3)Constant Variance. The variance of errors is constant across the predicted values. 4)Normality of Residuals. The error terms should be normally distributed with mean 0.

```{r}
# Correct model using your data frame name
model_full <- lm(price ~ mileage + engine_power + paint_color + model_key, data = dat_bmw)

# View model results
summary(model_full)

# Generate diagnostic plots
par(mfrow = c(2, 2))  # Arrange plots in a 2x2 grid
plot(model_full)
par(mfrow = c(1, 1))  # Reset layout

# Q-Q Plot for Normality
qqnorm(resid(model_full))
qqline(resid(model_full), col = "red")

# Shapiro-Wilk test for normality
shapiro.test(resid(model_full))  


# Install 'car' package if necessary
# install.packages("car")

library(car)
vif(model_full)  # Compute Variance Inflation Factor (VIF)

# Compute Cook’s Distance
cooksd <- cooks.distance(model_full)

# Plot Cook’s Distance
plot(cooksd, type = "h", main = "Cook's Distance")
abline(h = 4/(nrow(dat_bmw)), col = "red")  # Threshold line

```

### Quesion 1 revised

#### Fit the baseline model

```{r}
baseline_model <- lm(price ~ mileage + engine_power, data = dat_bmw_clean)
summary(baseline_model)
```

#### Diagonostic of the baseline model

```{r}
### Function to create diagnostic plots
diagPlot<-function(model){
    p1<-ggplot(model, aes(.fitted, .resid))+geom_point()
    p1<-p1+stat_smooth(method="loess")+geom_hline(yintercept=0, col="red", linetype="dashed")
    p1<-p1+xlab("Fitted values")+ylab("Residuals")
    p1<-p1+ggtitle("Residual vs Fitted Plot")+theme_bw()
    
    p2 <- ggplot(model, aes(sample = .stdresid)) +
      stat_qq() +
      stat_qq_line() +
      xlab("Theoretical Quantiles") +
      ylab("Standardized Residuals") +
      ggtitle("Normal Q-Q") +
      theme_bw()
    
    p3<-ggplot(model, aes(.fitted, sqrt(abs(.stdresid))))+geom_point(na.rm=TRUE)
    p3<-p3+stat_smooth(method="loess", na.rm = TRUE)+xlab("Fitted Value")
    p3<-p3+ylab(expression(sqrt("|Standardized residuals|")))
    p3<-p3+ggtitle("Scale-Location")+theme_bw()
    
    p5<-ggplot(model, aes(.hat, .stdresid))+geom_point(na.rm=TRUE)
    p5<-p5+stat_smooth(method="loess", na.rm=TRUE)
    p5<-p5+xlab("Leverage")+ylab("Standardized Residuals")
    p5<-p5+ggtitle("Residual vs Leverage Plot")
    p5<-p5+scale_size_continuous("Cook's Distance", range=c(1,5))
    p5<-p5+theme_bw()+theme(legend.position="bottom")
    
    return(list(rvfPlot=p1, qqPlot=p2, sclLocPlot=p3, rvlevPlot=p5))
}
```

```{r}
diagPlot(baseline_model)
```

#### Added Variable Plots

```{r}
feature_model <- lm(price ~ mileage + engine_power + feature_1 + feature_2 + feature_3 + feature_4 + feature_5 + feature_6 + feature_7 + feature_8, data = dat_bmw_clean)
avPlots(feature_model)
```

```{r}
Full_model <- lm(price ~ mileage + engine_power + model_series, data = dat_bmw_clean)
summary(Full_model)
avPlots(Full_model)
#diagPlot(Full_model)

```

Q2: Does our model predicts data well?

```{r}
dat_bmw2 <- dat_bmw_clean[complete.cases(dat_bmw_clean), c("price", "mileage", "engine_power", "paint_color", "model_key", paste0("feature_", 1:8))] #4840 --> 4838
##re-group model key variable due to its messy classification
dat_bmw2$model_key2 <- ifelse(dat_bmw2$model_key %like% "X", "X", 
                          ifelse(dat_bmw2$model_key %like% "Z", "Z",
                                 ifelse(dat_bmw2$model_key %like% "M", "M",
                                        ifelse(dat_bmw2$model_key %like% "i", "i", "others"
                                        ))))
```

#### randomly divided samples into training and testing data

```{r}
set.seed(2025)
group <- split(sample(1:nrow(dat_bmw2)),rep(1:2, times=c(nrow(dat_bmw2)/2, nrow(dat_bmw2)/2)))

dat_train <- dat_bmw2[group$`1`, ]
dat_test <- dat_bmw2[group$`2`, ]

## fit mod2 to the training data ##
model_full2 <- lm(price ~ mileage + engine_power + paint_color + model_key2, data = dat_bmw2)
summary(model_full2)
model_full_train <- lm(price ~ mileage + engine_power + paint_color + model_key2, data = dat_train)
summary(model_full_train)
```

#### elastic net to filter feature_1 - feature_8 columns

```{r}
## transfer categorical to dummy variables
X_train <- model.matrix(price ~ ., data = dat_train[,-5])[,-c(1)] ## remove intercept

cvfit_elas <- cv.glmnet(X_train, dat_train$price, alpha = 0.5,
                        penalty.factor = c(rep(0, 11),rep(1,8),rep(0,4)))
cvfit_elas$lambda.min #14.58556
cvfit_elas$lambda.1se #344.8741

model_elas0 <- glmnet(X_train, dat_train$price, alpha = 0.5, lambda = cvfit_elas$lambda.min,
               penalty.factor = c(rep(0, 11),rep(1,8),rep(0,4)))

coef_model_elas <- data.frame(matrix(predict(model_elas0,type="coef"))) %>% mutate("predictor" = predict(model_elas0,type="coef")@Dimnames[[1]])
colnames(coef_model_elas)[1] <- "beta"
```

#### build mode with selected features

```{r}
model_2 <- lm(price ~ mileage + engine_power + paint_color + model_key2 + feature_1 + feature_2 + feature_3 + feature_4 + feature_5 + feature_6 + feature_7 + feature_8, data = dat_train)
summary(model_2)

#### comparison between model_full_train and model 2 ####
anova(model_full_train, model_2)
```

#### performance on predicting

```{r}
#### predict new data using model_full_train and model 2 ####
dat_test$price_new1 <- predict(model_full_train, newdata = dat_test[,-1])
dat_test$price_new2 <- predict(model_2, newdata = dat_test[,-c(1,15)])


#### performance on predicting ####
dat_predict <- dat_test[,c("price", "mileage", "price_new1", "price_new2")]
## R^2
fun_r2 <- function(obe, pred){
  rss <- sum((obe-pred)^2)
  sst <- sum((obe-mean(obe))^2)
  r2 <- 1 - rss/sst
  
  return(r2)
}
fun_r2(dat_predict$price, dat_predict$price_new1)
fun_r2(dat_predict$price, dat_predict$price_new2)

## MSE
fun_mse <- function(obe, pred){
  mse <- (sum((obe-pred)^2))/length(pred)
  
  return(mse)
}

fun_mse(dat_predict$price, dat_predict$price_new1)
fun_mse(dat_predict$price, dat_predict$price_new2)

## MAE
fun_mae <- function(obe, pred){
  mse <- (sum(abs(obe-pred)))/length(pred)
  
  return(mse)
}
fun_mae(dat_predict$price, dat_predict$price_new1)
fun_mae(dat_predict$price, dat_predict$price_new2)

summary(dat_predict$price)
summary(dat_predict$price_new1)
summary(dat_predict$price_new2)
```

#### visualization

```{r}
dat_predict$price <- as.numeric(dat_predict$price)
dat_predict$mileage <- as.numeric(dat_predict$mileage)

ggplot() +
  geom_point(data = dat_predict, aes(x = mileage, y = price, color = "Observation"), size = 0.5) +
  geom_point(data = dat_predict, aes(x = mileage, y = price_new1, color = "Model 1"), alpha = 0.3, size = 0.5) +
  geom_point(data = dat_predict, aes(x = mileage, y = price_new2, color = "Model 2"), alpha = 0.3, size = 0.5) +
  labs(title = "Prediction of Price with primary and sensitivity analysis") +
  xlab("Mileage") +
  ylab("Price") +
  scale_color_manual(name = element_blank(), values = c("Observation" = "grey60", "Model 1" = "blue", "Model 2" = "red")) +
  theme_bw()
```

### Which models retain value the best

We want to find which models do not have a significantly negative relationship between price and age

```{r}
dat_bmw_clean <- dat_bmw_clean|>
  mutate(age = 2018 - year(as.Date(registration_date, format = "%m/%d/%Y")))|>
  mutate(series = substr(model_key, 1, 1))

# remove series with only 1 or 2 cars belonging
series_types = c(unique(dat_bmw_clean$series)[1:7], unique(dat_bmw_clean$series)[8:10])

coefs = c()

for(i in seq(1:length(series_types))){
  temp.df <- dat_bmw_clean|>
    filter(series == series_types[i])
  temp.lm <- lm(log(price) ~ age, data = temp.df)
  coefs = c(coefs, coef(temp.lm)[2])
  
}
rm(temp.df)
rm(temp.lm)

coefs
```

Although we would need to take a closer look at the standard errors for B1 estimates calculated, the results suggest that 100 and 300 series depreciate in value the slowest as their age increases.

### Appendix

The following data chunk was used to create summary statistics for the data overview section

```{r}
length(unique(dat_bmw$model_key))
unique(dat_bmw$paint_color)
unique(dat_bmw$fuel)

range(as.Date(dat_bmw$registration_date, format = "%m/%d/%Y"), na.rm=T)
range(as.Date(dat_bmw$sold_at, format = "%m/%d/%Y"), na.rm=T)

min(dat_bmw$mileage)
min(dat_bmw$engine_power)
min(dat_bmw$price)

median(dat_bmw$price)
range(dat_bmw$engine_power)

mean(dat_bmw$feature_1 == T)
mean(dat_bmw$feature_2 == T)
mean(dat_bmw$feature_3 == T)
mean(dat_bmw$feature_4 == T)
mean(dat_bmw$feature_5 == T)
mean(dat_bmw$feature_6 == T)
mean(dat_bmw$feature_7 == T)
mean(dat_bmw$feature_8 == T)

coef(lm(dat_bmw$price ~ dat_bmw$feature_7))

max(dat_bmw$mileage)

```
