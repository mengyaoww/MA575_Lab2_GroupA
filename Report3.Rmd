---
title: "Report3"
subtitle: "Lab2 Group A: Lee, Joshua; Liu, Kaiyi; Pulsone, Nathaniel; Wang, Mengyao; Xu, Zexian; Yang, Xiaojing"
output: pdf_document
subparagraph: yes
header-includes:
  \usepackage{titlesec}
  \titlespacing{\title}{0pt}{\parskip}{-\parskip}
  \usepackage{titling}
  \setlength{\droptitle}{-10em}
---

\vspace{-10truemm}

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,    # Show the R code in the final output.
  fig.pos = "!h"  # Suggests to position figures "here" in the output.
)
```

### Loading the Library and DataSet

```{r message=FALSE}
mylibrary <- c("tidyverse", "cowplot", "GGally", "MASS", "ggplot2", "glmnet", "data.table", "car", "zoo", "TSA", "tseries")
invisible(lapply(mylibrary, library, character.only = TRUE))
### load the data
dat_bmw <- read.csv("BMWpricing_updated.csv")
```

### Data Overview

Sourced from Kaggle, the dataset provides information on roughly 5000 used BMW cars sold in a business-to-business auction. Notable variables included are the price, car model, color, mileage, engine power, and various categorical descriptors.

```{r, echo = F}
par(mfrow=c(2,2))
dat_bmw[,c("price", "mileage", "engine_power")]|>
  pivot_longer(everything(), values_to = "Value", names_to = "Variable") |>
  ggplot() + geom_boxplot(aes(x=Variable, y=Value, color=Variable)) +
  labs(title = "Boxplots for Continuous Variables (Raw)")

dat_bmw|>
  ggplot() + geom_density(aes(x=price)) +
  labs(title = "Density of the Variable 'Price'")

dat_bmw|>
  ggplot(aes(x = as.Date(registration_date, format = "%m/%d/%Y"), y = price)) +
  geom_point() + 
  labs(title = "Price by Vehicle Registration Date") +
  xlab("Registration Date")

dat_bmw|>
  ggplot(aes(x = as.Date(sold_at, format = "%m/%d/%Y"))) +
  geom_bar() + 
  labs(title = "Sales by Date of Auction") +
  xlab("Date Sold") +
  ylab("Number of Cars Auctioned")
  
```

From the graphs above, along with analysis that can be found in the Appendix section, we can see that the cars from the auction were all registered between March 1990 and November 2017, and the auction took place from January to September 2018. The cars spanned 75 different BMW models, 10 different colors, and 4 different fuel types.

The pricing of the cars is most concentrated around 15000, with a median price of 14200. The distribution of price is skewed heavily to the right. Because price is our main response variable, we will see that this causes the residuals of the constructed models to be right-skewed as well. The skewness of the variables can be somewhat remedied with a log-transformation on price.

Although it is hard to identify the feature variables in the dataset, we can see That most of the vehicles posses features 2 and 7, while most do not have features 3, 4 and 6. Each of the features when present tend to increase the price by `$5000` to `$10000`, except for feature 7, which actually decreases the price by about `$185`

Lastly, there are a few unusual values and outliers to consider in the dataset. There are two cars in the dataset that were sold for more than `$100,000` which is unusual enough to provide a high leverage and skew our constructed models. For this reason, we will remove that observation from the dataset. In addition, there is a single car with over 1 million miles on it, which we speculate was an error in data entry, so we will also remove this from the dataset. Finally, there are a number of observations with a negative mileage, or an engine power of zero. These values are also either errors in data entry, or indicative of a special case, such as scrapped or salvaged car. For this reason, we will not remove the observations, but instead set the negative and zero values to NA.

```{r}
dat_bmw_clean <- dat_bmw|>
  filter(mileage < 500000 & mileage > 0)|>
  filter(price < 100000)|>
  filter(engine_power != 0)
  
head(dat_bmw_clean)
```

```{R}
### Create Model series variable
dat_bmw_clean <- dat_bmw_clean %>%
  mutate(model_series = case_when(
    grepl("^1", model_key) ~ "1_Series",
    grepl("^2", model_key) ~ "2_Series",
    grepl("^3", model_key) ~ "3_Series",
    grepl("^4", model_key) ~ "4_Series",
    grepl("^5", model_key) ~ "5_Series",
    grepl("^7", model_key) ~ "7_Series",
    grepl("^M|M$", model_key) ~ "M_Power",
    model_key %in% c("X1") ~ "X1",
    model_key %in% c("X3") ~ "X3",
    model_key %in% c("X5") ~ "X5",
    model_key %in% c("X6") ~ "X6",
    TRUE ~ "Other"
  ))
```

### Model

Q1: Which factor among mileage, engine power, color, model, and other categorical variables impacts the price of the car the most?

We have selected a set of variables: Mileage(numeric), Engine Power(numeric), color(categorical factor), Model(categorical factor), and other categorical factors.

We decided to fit a multiple linear regression. Price = $\beta_0$ + $\beta_1$(Mileage) + $\beta_2$(Engine Power) + ... = e,

The assumptions are: 1) Linearity.The relationship between each numeric predictor and the price is assumed to be linear. 2)Independence of Observations. Each observations should be independent of others. 3)Constant Variance. The variance of errors is constant across the predicted values. 4)Normality of Residuals. The error terms should be normally distributed with mean 0.

```{r}
# Correct model using your data frame name
model_full <- lm(price ~ mileage + engine_power + paint_color + model_key, data = dat_bmw)

# View model results
summary(model_full)

# Generate diagnostic plots
par(mfrow = c(2, 2))  # Arrange plots in a 2x2 grid
plot(model_full)
par(mfrow = c(1, 1))  # Reset layout

# Q-Q Plot for Normality
qqnorm(resid(model_full))
qqline(resid(model_full), col = "red")

# Shapiro-Wilk test for normality
shapiro.test(resid(model_full))  


# Install 'car' package if necessary
# install.packages("car")

library(car)
vif(model_full)  # Compute Variance Inflation Factor (VIF)

# Compute Cook’s Distance
cooksd <- cooks.distance(model_full)

# Plot Cook’s Distance
plot(cooksd, type = "h", main = "Cook's Distance")
abline(h = 4/(nrow(dat_bmw)), col = "red")  # Threshold line

```

### Quesion 1 revised

#### Fit the baseline model

```{r}
baseline_model <- lm(price ~ mileage + engine_power, data = dat_bmw_clean)
summary(baseline_model)
```

#### Diagonostic of the baseline model

```{r}
### Function to create diagnostic plots
diagPlot<-function(model){
    p1<-ggplot(model, aes(.fitted, .resid))+geom_point()
    p1<-p1+stat_smooth(method="loess")+geom_hline(yintercept=0, col="red", linetype="dashed")
    p1<-p1+xlab("Fitted values")+ylab("Residuals")
    p1<-p1+ggtitle("Residual vs Fitted Plot")+theme_bw()
    
    p2 <- ggplot(model, aes(sample = .stdresid)) +
      stat_qq() +
      stat_qq_line() +
      xlab("Theoretical Quantiles") +
      ylab("Standardized Residuals") +
      ggtitle("Normal Q-Q") +
      theme_bw()
    
    p3<-ggplot(model, aes(.fitted, sqrt(abs(.stdresid))))+geom_point(na.rm=TRUE)
    p3<-p3+stat_smooth(method="loess", na.rm = TRUE)+xlab("Fitted Value")
    p3<-p3+ylab(expression(sqrt("|Standardized residuals|")))
    p3<-p3+ggtitle("Scale-Location")+theme_bw()
    
    p5<-ggplot(model, aes(.hat, .stdresid))+geom_point(na.rm=TRUE)
    p5<-p5+stat_smooth(method="loess", na.rm=TRUE)
    p5<-p5+xlab("Leverage")+ylab("Standardized Residuals")
    p5<-p5+ggtitle("Residual vs Leverage Plot")
    p5<-p5+scale_size_continuous("Cook's Distance", range=c(1,5))
    p5<-p5+theme_bw()+theme(legend.position="bottom")
    
    #return(list(rvfPlot=p1, qqPlot=p2, sclLocPlot=p3, rvlevPlot=p5))
    plot_grid(p1, p2, p3, p5, align = "h")
}
```

```{r}
diagPlot(baseline_model)
```

#### Added Variable Plots

```{r}
feature_model <- lm(price ~ mileage + engine_power + feature_1 + feature_2 + feature_3 + feature_4 + feature_5 + feature_6 + feature_7 + feature_8, data = dat_bmw_clean)
avPlots(feature_model)
```

```{r}
Full_model <- lm(price ~ mileage + engine_power + model_series, data = dat_bmw_clean)
summary(Full_model)
avPlots(Full_model)
diagPlot(Full_model)

```


### model refinement

Based on the curved line observed in the "Residual vs Fitted" Plot in the "Full model" and the quadratic relationship between price and mileage, we add mileage^2 to the model.  
The fitted line in the "Residual vs Fitted" Plot is not as quadratic as it in the "Full_model". The deviated tail may be caused by potential outliers.

```{r}
ggplot(data = dat_bmw_clean, mapping = aes(x = price, y = mileage)) +
  geom_point(color = "blue", size = 0.5) +
  theme_bw()

Full_model2 <- lm(price ~ mileage + engine_power + model_series + I(mileage^(1/2)), data = dat_bmw_clean)
summary(Full_model2)
anova(Full_model, Full_model2)

diagPlot(Full_model2)
```

#### add interaction term
```{r}
Full_model3 <- lm(price ~ mileage + engine_power + model_series + I(mileage^(1/2)) + mileage:engine_power, data = dat_bmw_clean)
summary(Full_model3)
anova(Full_model2, Full_model3)

diagPlot(Full_model3)
```


### predicting performance

#### randomly divided samples into training and testing data

```{r}
set.seed(2025)
group <- split(sample(1:nrow(dat_bmw_clean)),rep(1:2, times=c(nrow(dat_bmw_clean)/2, nrow(dat_bmw_clean)/2)))

dat_train <- dat_bmw_clean[group$`1`, ]
dat_test <- dat_bmw_clean[group$`2`, ]

## fit Full_model4 to the training data ##
Full_model3_train <- lm(price ~ mileage + engine_power + model_series + I(mileage^(1/2)) + mileage:engine_power, data = dat_train)
summary(Full_model3_train)
```

#### performance on predicting

```{r}
#### predict new data using model_full_train and model 2 ####
dat_test$price_new <- predict(Full_model3_train, newdata = dat_test[,-17])

#### performance on predicting ####
dat_predict <- dat_test[,c("price", "mileage", "price_new")]
## R^2
fun_r2 <- function(obe, pred){
  rss <- sum((obe-pred)^2)
  sst <- sum((obe-mean(obe))^2)
  r2 <- 1 - rss/sst
  
  return(r2)
}
fun_r2(dat_predict$price, dat_predict$price_new)

## MSE
fun_mse <- function(obe, pred){
  mse <- (sum((obe-pred)^2))/length(pred)
  
  return(mse)
}
fun_mse(dat_predict$price, dat_predict$price_new)

## MAE
fun_mae <- function(obe, pred){
  mse <- (sum(abs(obe-pred)))/length(pred)
  
  return(mse)
}
fun_mae(dat_predict$price, dat_predict$price_new)

summary(dat_predict$price)
summary(dat_predict$price_new)
```

#### visualization

```{r}
dat_predict$price <- as.numeric(dat_predict$price)
dat_predict$mileage <- as.numeric(dat_predict$mileage)

ggplot() +
  geom_point(data = dat_predict, aes(x = mileage, y = price, color = "Observation"), size = 0.5) +
  geom_point(data = dat_predict, aes(x = mileage, y = price_new, color = "Prediction"), alpha = 0.5, size = 0.5) +
  labs(title = "Prediction of Price with primary and sensitivity analysis") +
  xlab("Mileage") +
  ylab("Price") +
  scale_color_manual(name = element_blank(), values = c("Observation" = "grey60", "Prediction" = "blue")) +
  theme_bw()
```


### Question 3
```{r}
# create age variable
dat_bmw_clean <- dat_bmw_clean|>
  mutate(age = 2018 - year(as.Date(registration_date, format = "%m/%d/%Y")) - 
           0.25 * quarter(as.Date(registration_date, format = "%m/%d/%Y")))

# determine models with sufficient data
model_candidates <- dat_bmw_clean|>
  group_by(model_key)|>
  summarize(n = n())|>
  filter(n > 100)
model_candidates
model_candidates$model_key

# narrow down number of models considered
coefs = c()
confintlbs = c()

for(i in seq(1:length(model_candidates$model_key))){
  temp.df <- dat_bmw_clean|>
    filter(model_key == model_candidates$model_key[i])
  temp.lm <- lm(log(price) ~ age, data = temp.df)
  coefs = c(coefs, coef(temp.lm)[2])
  confintlbs = c(confintlbs, confint(temp.lm)[2])
}

rm(temp.df)
rm(temp.lm)

results = cbind(coefs, confintlbs, model_candidates$model_key)
results

plot(y = dat_bmw_clean$price[which(dat_bmw_clean$model_key == "116")],
     x = dat_bmw_clean$age[which(dat_bmw_clean$model_key == "116")])

plot(y = dat_bmw_clean$price[which(dat_bmw_clean$model_key == "318")],
     x = dat_bmw_clean$age[which(dat_bmw_clean$model_key == "318")])

plot(y = dat_bmw_clean$price[which(dat_bmw_clean$model_key == "X1")],
     x = dat_bmw_clean$age[which(dat_bmw_clean$model_key == "X1")])

plot(y = dat_bmw_clean$price[which(dat_bmw_clean$model_key == "X3")],
     x = dat_bmw_clean$age[which(dat_bmw_clean$model_key == "X3")])

# create time series objects for each model

avg_prices <- dat_bmw_clean |>
  filter(model_key == "116"|
         model_key == "318"|
         model_key == "X1"|
         model_key == "X3")|>
  group_by(age, model_key)|>
  summarize(avg_price = mean(price))|>
  arrange(model_key)

ts_116 = rep(NA, 4*(max(avg_prices$age[which(avg_prices$model_key == "116")])))
for(i in seq(1:length(avg_prices$age[which(avg_prices$model_key == "116")]))){
  t = 4 * avg_prices$age[which(avg_prices$model_key == "116")][i]
  ts_116[t] = avg_prices$avg_price[which(avg_prices$model_key == "116")][i]
}
ts_116 = na.approx(ts_116)
ts_318 = rep(NA, 4*(max(avg_prices$age[which(avg_prices$model_key == "318")])))
for(i in seq(1:length(avg_prices$age[which(avg_prices$model_key == "318")]))){
  t = 4 * avg_prices$age[which(avg_prices$model_key == "318")][i]
  ts_318[t] = avg_prices$avg_price[which(avg_prices$model_key == "318")][i]
}
ts_318 = na.approx(ts_318)
ts_X1 = rep(NA, 4*(max(avg_prices$age[which(avg_prices$model_key == "X1")])))
for(i in seq(1:length(avg_prices$age[which(avg_prices$model_key == "X1")]))){
  t = 4 * avg_prices$age[which(avg_prices$model_key == "X1")][i]
  ts_X1[t] = avg_prices$avg_price[which(avg_prices$model_key == "X1")][i]
}
ts_X1 = na.approx(ts_X1)
ts_X3 = rep(NA, 4*(max(avg_prices$age[which(avg_prices$model_key == "X3")])))
for(i in seq(1:length(avg_prices$age[which(avg_prices$model_key == "X3")]))){
  t = 4 * avg_prices$age[which(avg_prices$model_key == "X3")][i]
  ts_X3[t] = avg_prices$avg_price[which(avg_prices$model_key == "X3")][i]
}
ts_X3 = na.approx(ts_X3)

ts_116 = ts(ts_116, frequency = 4)
ts_318 = ts(ts_318, frequency = 4)
ts_X1 = ts(ts_X1, frequency = 4)
ts_X3 = ts(ts_X3, frequency = 4)

adf.test(decompose(ts_116)$random[3:48]) # Result: series not should be integrated once, d = 0
model_116 = decompose(ts_116, type = "multiplicative")

adf.test(decompose(ts_318)$random[3:91]) # Result: series should not be integrated once, d = 0
model_318 = decompose(ts_318)

adf.test(decompose(ts_X1)$random[3:28]) # Result: series should not be integrated, d = 0
model_X1 = decompose(ts_X1)

adf.test(decompose(ts_X3)$random[3:51]) # Result: series should not be integrated once, d = 0
model_X3 = decompose(ts_X3)

#Plot Time series data, ACF, PACF, and EACF for each model key

plot(model_116)
acf(model_116$random[3:47])
pacf(model_116$random[3:47])
eacf(model_116$random[3:47])

plot(model_318)
acf(model_318$random[3:91])
pacf(model_318$random[3:91])
eacf(model_318$random[3:91])

plot(model_X1)
acf(model_X1$random[3:26])
pacf(model_318$random[3:26])


plot(model_X3)
acf(model_X3$random[3:51])
pacf(model_X3$random[3:51])
eacf(model_X3$random[3:51])
```